{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bb3077-84c3-4244-bc5a-8bec49c58b40",
   "metadata": {},
   "source": [
    "# Crear SHP de EM_CONAGUA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fae67b5-c1b1-4aa3-9373-36a8c9dec9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████████████| 5445/5445 [02:08<00:00, 42.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Función para extraer datos climatológicos desde un archivo TXT\n",
    "def extraer_datos_climatologicos(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Encontrar el inicio de los datos climatológicos\n",
    "    inicio_datos = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith('Fecha'):\n",
    "            inicio_datos = i + 1\n",
    "            break\n",
    "\n",
    "    if inicio_datos is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extraer y procesar los datos climatológicos\n",
    "    data = []\n",
    "    for line in lines[inicio_datos:]:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 5:\n",
    "            try:\n",
    "                fecha = parts[0]\n",
    "                dia, mes, año = fecha.split('/')\n",
    "                dia = int(dia)\n",
    "                mes = int(mes)\n",
    "                año = int(año)\n",
    "                if año >= 2000 and año <= 2020:  # Filtrar por el rango de fechas\n",
    "                    precip = parts[1]\n",
    "                    tmax = parts[3] if parts[3] != 'Nulo' else None\n",
    "                    tmin = parts[4] if parts[4] != 'Nulo' else None\n",
    "                    data.append([dia, mes, año, precip, tmin, tmax])\n",
    "            except ValueError:\n",
    "                continue  # Ignorar líneas con formato de fecha incorrecto\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'])\n",
    "    df['Precip'] = pd.to_numeric(df['Precip'], errors='coerce')\n",
    "    df['TMin'] = pd.to_numeric(df['TMin'], errors='coerce')\n",
    "    df['TMax'] = pd.to_numeric(df['TMax'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para convertir los archivos TXT a CSV, solo para las estaciones en el rango de fechas deseado\n",
    "def convertir_txt_a_csv(carpeta_txt, carpeta_csv, metadatos_path):\n",
    "    # Leer los metadatos\n",
    "    metadatos = pd.read_csv(metadatos_path)\n",
    "    metadatos['fecha_inicial'] = pd.to_datetime(metadatos['fecha_inicial'])\n",
    "    metadatos['fecha_final'] = pd.to_datetime(metadatos['fecha_final'])\n",
    "    \n",
    "    # Filtrar estaciones por fecha\n",
    "    fecha_inicio = pd.Timestamp('2000-01-01')\n",
    "    fecha_fin = pd.Timestamp('2020-12-31')\n",
    "    metadatos_filtrados = metadatos[(metadatos['fecha_inicial'] <= fecha_inicio) & (metadatos['fecha_final'] >= fecha_fin)]\n",
    "    \n",
    "    # Obtener lista de estaciones filtradas en formato de 5 dígitos\n",
    "    estaciones_filtradas = metadatos_filtrados['estacion'].astype(str).str.zfill(5).values\n",
    "    \n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(carpeta_csv, exist_ok=True)\n",
    "    \n",
    "    # Obtener lista de archivos TXT en la carpeta y extraer los IDs de estación en formato de 5 dígitos\n",
    "    archivos_txt = [os.path.splitext(f)[0][3:].zfill(5) for f in os.listdir(carpeta_txt) if f.startswith('dia') and f.endswith('.TXT')]\n",
    "    \n",
    "    # Barra de progreso\n",
    "    for archivo in tqdm(os.listdir(carpeta_txt), desc=\"Procesando archivos\"):\n",
    "        if archivo.startswith('dia') and archivo.endswith('.TXT'):\n",
    "            file_path = os.path.join(carpeta_txt, archivo)\n",
    "            \n",
    "            # Obtener ID de la estación desde el nombre del archivo y ajustar al formato de 5 dígitos\n",
    "            estacion_id = os.path.splitext(archivo)[0][3:].zfill(5)\n",
    "            \n",
    "            # Verificar si la estación está en los metadatos filtrados\n",
    "            if estacion_id in estaciones_filtradas:\n",
    "                # Extraer datos climatológicos\n",
    "                df = extraer_datos_climatologicos(file_path)\n",
    "                \n",
    "                # Verificar que el DataFrame contiene datos desde el 1 de enero de 2000 hasta el 31 de diciembre de 2020\n",
    "                if not df.empty and df['Año'].min() <= 2000 and df['Año'].max() >= 2020:\n",
    "                    archivo_csv = os.path.splitext(archivo)[0] + '.csv'\n",
    "                    df.to_csv(os.path.join(carpeta_csv, archivo_csv), index=False)\n",
    "\n",
    "# Parámetros de entrada\n",
    "carpeta_txt = '/Users/DiegoRB/Desktop/Proyectos/EM_Mex/txts' \n",
    "carpeta_csv = '/Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021'  \n",
    "metadatos_path = '/Users/DiegoRB/Desktop/Proyectos/EM_Mex/Metadatos/metadatos_final_2.csv' \n",
    "\n",
    "# Convertir los archivos\n",
    "convertir_txt_a_csv(carpeta_txt, carpeta_csv, metadatos_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6603f1-4703-4816-af15-60ac69b88add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de archivos en la carpeta es: 1058\n"
     ]
    }
   ],
   "source": [
    "# Ruta de la carpeta\n",
    "carpeta = '/Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021'\n",
    "\n",
    "# Contar archivos en la carpeta\n",
    "cantidad_archivos = len([f for f in os.listdir(carpeta) if os.path.isfile(os.path.join(carpeta, f))])\n",
    "\n",
    "print(f\"La cantidad de archivos en la carpeta es: {cantidad_archivos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b432075-a735-48da-977b-ca1371121b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de estaciones con archivos CSV generados: 1058\n",
      "Cantidad de estaciones sin archivos CSV: 143\n",
      "Estaciones sin archivos CSV: ['01077', '01089', '02031', '02034', '02055', '02070', '02138', '02152', '04002', '04011', '04025', '04030', '04036', '04052', '05157', '05180', '06007', '06062', '08182', '09010', '09045', '10143', '12077', '12106', '12113', '12125', '13008', '13015', '13024', '13091', '13128', '13130', '13144', '14075', '14099', '14139', '14149', '14154', '14189', '14368', '15028', '15037', '15046', '15047', '15050', '15059', '15074', '15077', '15081', '15090', '15100', '15101', '15103', '15106', '15125', '15130', '15133', '15135', '15137', '15167', '15263', '15274', '15283', '16003', '16026', '16027', '16078', '16174', '16238', '16512', '17022', '17062', '17063', '17064', '17065', '17066', '18007', '18010', '18012', '18015', '18044', '18072', '18076', '19004', '19016', '19036', '19053', '19068', '19136', '19142', '19152', '19166', '19172', '20003', '20030', '20073', '20096', '20129', '20177', '20271', '20294', '20308', '20374', '20385', '20387', '20506', '21003', '21009', '21031', '21032', '21034', '21046', '21050', '21067', '21083', '21094', '21107', '21123', '21132', '21158', '21185', '21189', '21190', '21198', '21207', '21210', '21212', '21217', '21244', '22016', '22063', '23158', '24025', '24056', '24081', '24091', '29016', '30054', '30067', '30178', '30327', '30337', '30361']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "metadatos = pd.read_csv(metadatos_path)\n",
    "\n",
    "# Convertir las fechas de operación a formato datetime\n",
    "metadatos['fecha_inicial'] = pd.to_datetime(metadatos['fecha_inicial'])\n",
    "metadatos['fecha_final'] = pd.to_datetime(metadatos['fecha_final'])\n",
    "\n",
    "# Definir el rango de fechas\n",
    "fecha_inicio = pd.Timestamp('2000-01-01')\n",
    "fecha_fin = pd.Timestamp('2020-12-31')\n",
    "\n",
    "# Filtrar estaciones que cumplen con el criterio\n",
    "metadatos_filtrados = metadatos[(metadatos['fecha_inicial'] <= fecha_inicio) & (metadatos['fecha_final'] >= fecha_fin)]\n",
    "estaciones_filtradas = metadatos_filtrados['estacion'].astype(str).str.zfill(5).values\n",
    "\n",
    "# Obtener lista de archivos CSV generados\n",
    "archivos_csv = [os.path.splitext(f)[0][3:].zfill(5) for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "\n",
    "# Verificar qué estaciones tienen archivos CSV generados y cuáles faltan\n",
    "estaciones_con_archivo = [estacion for estacion in estaciones_filtradas if estacion in archivos_csv]\n",
    "estaciones_sin_archivo = [estacion for estacion in estaciones_filtradas if estacion not in archivos_csv]\n",
    "\n",
    "# Resultados\n",
    "print(f\"Cantidad de estaciones con archivos CSV generados: {len(estaciones_con_archivo)}\")\n",
    "print(f\"Cantidad de estaciones sin archivos CSV: {len(estaciones_sin_archivo)}\")\n",
    "print(\"Estaciones sin archivos CSV:\", estaciones_sin_archivo[:160])  # Mostrar las primeras 10 estaciones que faltan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15810bf-7704-4f04-b7d5-07e739a3b8fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01077',\n",
       " '01089',\n",
       " '02031',\n",
       " '02034',\n",
       " '02055',\n",
       " '02070',\n",
       " '02138',\n",
       " '02152',\n",
       " '04002',\n",
       " '04011',\n",
       " '04025',\n",
       " '04030',\n",
       " '04036',\n",
       " '04052',\n",
       " '05157',\n",
       " '05180',\n",
       " '06007',\n",
       " '06062',\n",
       " '08182',\n",
       " '09010',\n",
       " '09045',\n",
       " '10143',\n",
       " '12077',\n",
       " '12106',\n",
       " '12113',\n",
       " '12125',\n",
       " '13008',\n",
       " '13015',\n",
       " '13024',\n",
       " '13091',\n",
       " '13128',\n",
       " '13130',\n",
       " '13144',\n",
       " '14075',\n",
       " '14099',\n",
       " '14139',\n",
       " '14149',\n",
       " '14154',\n",
       " '14189',\n",
       " '14368',\n",
       " '15028',\n",
       " '15037',\n",
       " '15046',\n",
       " '15047',\n",
       " '15050',\n",
       " '15059',\n",
       " '15074',\n",
       " '15077',\n",
       " '15081',\n",
       " '15090',\n",
       " '15100',\n",
       " '15101',\n",
       " '15103',\n",
       " '15106',\n",
       " '15125',\n",
       " '15130',\n",
       " '15133',\n",
       " '15135',\n",
       " '15137',\n",
       " '15167',\n",
       " '15263',\n",
       " '15274',\n",
       " '15283',\n",
       " '16003',\n",
       " '16026',\n",
       " '16027',\n",
       " '16078',\n",
       " '16174',\n",
       " '16238',\n",
       " '16512',\n",
       " '17022',\n",
       " '17062',\n",
       " '17063',\n",
       " '17064',\n",
       " '17065',\n",
       " '17066',\n",
       " '18007',\n",
       " '18010',\n",
       " '18012',\n",
       " '18015',\n",
       " '18044',\n",
       " '18072',\n",
       " '18076',\n",
       " '19004',\n",
       " '19016',\n",
       " '19036',\n",
       " '19053',\n",
       " '19068',\n",
       " '19136',\n",
       " '19142',\n",
       " '19152',\n",
       " '19166',\n",
       " '19172',\n",
       " '20003',\n",
       " '20030',\n",
       " '20073',\n",
       " '20096',\n",
       " '20129',\n",
       " '20177',\n",
       " '20271',\n",
       " '20294',\n",
       " '20308',\n",
       " '20374',\n",
       " '20385',\n",
       " '20387',\n",
       " '20506',\n",
       " '21003',\n",
       " '21009',\n",
       " '21031',\n",
       " '21032',\n",
       " '21034',\n",
       " '21046',\n",
       " '21050',\n",
       " '21067',\n",
       " '21083',\n",
       " '21094',\n",
       " '21107',\n",
       " '21123',\n",
       " '21132',\n",
       " '21158',\n",
       " '21185',\n",
       " '21189',\n",
       " '21190',\n",
       " '21198',\n",
       " '21207',\n",
       " '21210',\n",
       " '21212',\n",
       " '21217',\n",
       " '21244',\n",
       " '22016',\n",
       " '22063',\n",
       " '23158',\n",
       " '24025',\n",
       " '24056',\n",
       " '24081',\n",
       " '24091',\n",
       " '29016',\n",
       " '30054',\n",
       " '30067',\n",
       " '30178',\n",
       " '30327',\n",
       " '30337',\n",
       " '30361']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estaciones_sin_archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f5f20c7-2c52-483f-8eb1-cc0b6cf1698f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando estaciones faltantes: 100%|███████| 143/143 [00:05<00:00, 24.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de estaciones omitidas: 143\n",
      "Estación: 01077, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 01089, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02031, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02034, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02055, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02070, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02138, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 02152, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 04002, Problema: Datos no cubren todo el rango de fechas\n",
      "Estación: 04011, Problema: Datos no cubren todo el rango de fechas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Función para extraer datos climatológicos desde un archivo TXT\n",
    "def extraer_datos_climatologicos(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Encontrar el inicio de los datos climatológicos\n",
    "    inicio_datos = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith('Fecha'):\n",
    "            inicio_datos = i + 1\n",
    "            break\n",
    "\n",
    "    if inicio_datos is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extraer y procesar los datos climatológicos\n",
    "    data = []\n",
    "    for line in lines[inicio_datos:]:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 5:\n",
    "            try:\n",
    "                fecha = parts[0]\n",
    "                dia, mes, año = fecha.split('/')\n",
    "                dia = int(dia)\n",
    "                mes = int(mes)\n",
    "                año = int(año)\n",
    "                if año >= 2000 and año <= 2020:  # Filtrar por el rango de fechas\n",
    "                    precip = parts[1]\n",
    "                    tmax = parts[3] if parts[3] != 'Nulo' else None\n",
    "                    tmin = parts[4] if parts[4] != 'Nulo' else None\n",
    "                    data.append([dia, mes, año, precip, tmin, tmax])\n",
    "            except ValueError:\n",
    "                continue  # Ignorar líneas con formato de fecha incorrecto\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'])\n",
    "    df['Precip'] = pd.to_numeric(df['Precip'], errors='coerce')\n",
    "    df['TMin'] = pd.to_numeric(df['TMin'], errors='coerce')\n",
    "    df['TMax'] = pd.to_numeric(df['TMax'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para verificar las estaciones faltantes\n",
    "def verificar_estaciones_omitidas(carpeta_txt, estaciones_sin_archivo):\n",
    "    problemas_estaciones = {}\n",
    "    for estacion_id in tqdm(estaciones_sin_archivo, desc=\"Verificando estaciones faltantes\"):\n",
    "        file_path = os.path.join(carpeta_txt, f'dia{estacion_id}.TXT')\n",
    "        if os.path.exists(file_path):\n",
    "            df = extraer_datos_climatologicos(file_path)\n",
    "            if df.empty:\n",
    "                problemas_estaciones[estacion_id] = \"Datos vacíos o fuera del rango de fechas\"\n",
    "            elif df['Año'].min() > 2000 or df['Año'].max() < 2020:\n",
    "                problemas_estaciones[estacion_id] = \"Datos no cubren todo el rango de fechas\"\n",
    "            else:\n",
    "                problemas_estaciones[estacion_id] = \"Otro problema desconocido\"\n",
    "        else:\n",
    "            problemas_estaciones[estacion_id] = \"Archivo TXT no encontrado\"\n",
    "    \n",
    "    return problemas_estaciones\n",
    "\n",
    "\n",
    "# Leer los metadatos y obtener estaciones filtradas\n",
    "metadatos = pd.read_csv(metadatos_path)\n",
    "metadatos['fecha_inicial'] = pd.to_datetime(metadatos['fecha_inicial'])\n",
    "metadatos['fecha_final'] = pd.to_datetime(metadatos['fecha_final'])\n",
    "fecha_inicio = pd.Timestamp('2000-01-01')\n",
    "fecha_fin = pd.Timestamp('2020-12-31')\n",
    "metadatos_filtrados = metadatos[(metadatos['fecha_inicial'] <= fecha_inicio) & (metadatos['fecha_final'] >= fecha_fin)]\n",
    "estaciones_filtradas = metadatos_filtrados['estacion'].astype(str).str.zfill(5).values\n",
    "\n",
    "# Obtener lista de archivos CSV generados\n",
    "archivos_csv = [os.path.splitext(f)[0][3:].zfill(5) for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "\n",
    "# Verificar qué estaciones tienen archivos CSV generados y cuáles faltan\n",
    "estaciones_sin_archivo = [estacion for estacion in estaciones_filtradas if estacion not in archivos_csv]\n",
    "\n",
    "# Verificar las estaciones faltantes\n",
    "problemas_estaciones = verificar_estaciones_omitidas(carpeta_txt, estaciones_sin_archivo)\n",
    "\n",
    "# Mostrar algunos resultados\n",
    "print(f\"Cantidad de estaciones omitidas: {len(problemas_estaciones)}\")\n",
    "for estacion, problema in list(problemas_estaciones.items())[:10]:\n",
    "    print(f\"Estación: {estacion}, Problema: {problema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8689d491-55bb-4654-b102-8a1169d27842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas de /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia08142.csv:\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0  -3.0  12.6\n",
      "1    2    1  2000     0.0  -6.0  14.4\n",
      "2    3    1  2000     0.0  -9.0  17.6\n",
      "3    4    1  2000     0.0 -11.0  15.8\n",
      "4    5    1  2000     0.0 -11.4  18.0\n",
      "Columnas presentes: Index(['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'], dtype='object')\n",
      "Primeras filas de /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia08156.csv:\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0   6.0  26.0\n",
      "1    2    1  2000     0.0  -1.0  27.0\n",
      "2    3    1  2000     0.0  -1.0  27.0\n",
      "3    4    1  2000     0.0  -5.0  20.0\n",
      "4    5    1  2000     0.0  -4.0  13.0\n",
      "Columnas presentes: Index(['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'], dtype='object')\n",
      "Primeras filas de /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia19067.csv:\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0   3.0  23.0\n",
      "1    2    1  2000     0.0   3.0  19.0\n",
      "2    3    1  2000     0.0   0.0  17.0\n",
      "3    4    1  2000     0.0  -4.0  15.0\n",
      "4    5    1  2000     0.0   0.0  12.0\n",
      "Columnas presentes: Index(['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'], dtype='object')\n",
      "Primeras filas de /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia19098.csv:\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0  13.0  30.0\n",
      "1    2    1  2000     0.0  15.0  22.0\n",
      "2    3    1  2000     0.0  17.0  22.0\n",
      "3    4    1  2000     0.0   9.0  19.0\n",
      "4    5    1  2000     0.0   2.0  19.0\n",
      "Columnas presentes: Index(['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'], dtype='object')\n",
      "Primeras filas de /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia22026.csv:\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0   1.0  26.0\n",
      "1    2    1  2000     0.0   4.0  26.0\n",
      "2    3    1  2000     0.0   6.0  21.0\n",
      "3    4    1  2000     0.0   4.0  26.0\n",
      "4    5    1  2000     0.0   4.0  20.0\n",
      "Columnas presentes: Index(['Dia', 'Mes', 'Año', 'Precip', 'TMin', 'TMax'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Función para revisar la estructura de datos\n",
    "def revisar_estructura(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Primeras filas de {file_path}:\")\n",
    "        print(df.head())\n",
    "        print(f\"Columnas presentes: {df.columns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer {file_path}: {e}\")\n",
    "\n",
    "# Revisar la estructura de algunos archivos CSV\n",
    "archivos_csv = [f for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "for archivo in archivos_csv[:5]:  # Revisar solo los primeros 5 archivos\n",
    "    file_path = os.path.join(carpeta_csv, archivo)\n",
    "    revisar_estructura(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b4451f0-4cd5-46bd-b5bc-6e0967d71a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia08142.csv\n",
      "Valores únicos en 'Dia': [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31]\n",
      "Valores únicos en 'Mes': [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Valores únicos en 'Año': [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
      " 2014 2015 2016 2017 2018 2019 2020]\n",
      "Después de conversión:\n",
      "Valores únicos en 'Dia': [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31]\n",
      "Valores únicos en 'Mes': [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Valores únicos en 'Año': [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
      " 2014 2015 2016 2017 2018 2019 2020]\n",
      "Error procesando /Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021/dia08142.csv: to assemble mappings requires at least that [year, month, day] be specified: [day,month,year] is missing\n",
      "   Dia  Mes   Año  Precip  TMin  TMax\n",
      "0    1    1  2000     0.0  -3.0  12.6\n",
      "1    2    1  2000     0.0  -6.0  14.4\n",
      "2    3    1  2000     0.0  -9.0  17.6\n",
      "3    4    1  2000     0.0 -11.0  15.8\n",
      "4    5    1  2000     0.0 -11.4  18.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Función para agregar columna de fecha y guardar el CSV usando nombres de columna\n",
    "def agregar_columna_fecha(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if all(col in df.columns for col in ['Dia', 'Mes', 'Año']):\n",
    "            # Verificar valores de las columnas antes de convertir\n",
    "            print(f\"Procesando {file_path}\")\n",
    "            print(\"Valores únicos en 'Dia':\", df['Dia'].unique())\n",
    "            print(\"Valores únicos en 'Mes':\", df['Mes'].unique())\n",
    "            print(\"Valores únicos en 'Año':\", df['Año'].unique())\n",
    "            \n",
    "            # Convertir columnas a numéricas en caso de que haya errores de formato\n",
    "            df['Dia'] = pd.to_numeric(df['Dia'], errors='coerce')\n",
    "            df['Mes'] = pd.to_numeric(df['Mes'], errors='coerce')\n",
    "            df['Año'] = pd.to_numeric(df['Año'], errors='coerce')\n",
    "            \n",
    "            # Verificar nuevamente después de la conversión\n",
    "            print(\"Después de conversión:\")\n",
    "            print(\"Valores únicos en 'Dia':\", df['Dia'].unique())\n",
    "            print(\"Valores únicos en 'Mes':\", df['Mes'].unique())\n",
    "            print(\"Valores únicos en 'Año':\", df['Año'].unique())\n",
    "            \n",
    "            # Crear la columna de fecha\n",
    "            df['Fecha'] = pd.to_datetime(df[['Año', 'Mes', 'Dia']])\n",
    "            \n",
    "            # Reordenar las columnas para tener 'Fecha' al inicio\n",
    "            df = df[['Fecha'] + [col for col in df.columns if col != 'Fecha']]\n",
    "            df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            print(f\"Columnas faltantes en {file_path}. Columnas presentes: {df.columns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {file_path}: {e}\")\n",
    "        # Mostrar algunas filas del archivo problemático para depuración\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file_path).head()\n",
    "            print(df_sample)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error al leer el archivo {file_path} para depuración: {e2}\")\n",
    "\n",
    "# Parámetros de entrada\n",
    "carpeta_csv = '/Users/DiegoRB/Desktop/Proyectos/EM_Mex/csvs_2000_2021'   # Ajusta esta ruta según tu entorno\n",
    "\n",
    "# Agregar columna de fecha a los archivos CSV\n",
    "archivos_csv = [f for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "for archivo in archivos_csv[:1]:  # Revisar solo el primer archivo para diagnóstico\n",
    "    file_path = os.path.join(carpeta_csv, archivo)\n",
    "    agregar_columna_fecha(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "112d85e3-7479-42a4-94b8-f74bbb818631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agregando columna de fecha a CSVs: 100%|████| 1058/1058 [01:44<00:00, 10.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Función para agregar columna de fecha y guardar el CSV\n",
    "def agregar_columna_fecha(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if all(col in df.columns for col in ['Dia', 'Mes', 'Año']):\n",
    "            # Crear la columna de fecha concatenando las columnas Dia, Mes y Año\n",
    "            df['Fecha'] = pd.to_datetime(df['Año'].astype(str) + '-' + df['Mes'].astype(str) + '-' + df['Dia'].astype(str), errors='coerce')\n",
    "            # Reordenar las columnas para tener 'Fecha' al inicio\n",
    "            df = df[['Fecha'] + [col for col in df.columns if col != 'Fecha']]\n",
    "            df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            print(f\"Columnas faltantes en {file_path}. Columnas presentes: {df.columns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {file_path}: {e}\")\n",
    "        # Mostrar algunas filas del archivo problemático para depuración\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file_path).head()\n",
    "            print(df_sample)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error al leer el archivo {file_path} para depuración: {e2}\")\n",
    "\n",
    "# Agregar columna de fecha a los archivos CSV\n",
    "archivos_csv = [f for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "for archivo in tqdm(archivos_csv, desc=\"Agregando columna de fecha a CSVs\"):\n",
    "    file_path = os.path.join(carpeta_csv, archivo)\n",
    "    agregar_columna_fecha(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80849bc6-4160-4a78-93de-0b0ec3a07478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando fechas completas en CSVs: 100%|█| 1058/1058 [00:17<00:00, 59.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de archivos CSV con fechas incompletas: 996\n",
      "Archivos CSV con fechas incompletas: ['dia08142.csv', 'dia08156.csv', 'dia19067.csv', 'dia19098.csv', 'dia22026.csv', 'dia20143.csv', 'dia15305.csv', 'dia30185.csv', 'dia14033.csv', 'dia02137.csv']\n",
      "Fechas faltantes en dia08142.csv: DatetimeIndex(['2002-01-01', '2002-01-02', '2002-01-03', '2002-01-04',\n",
      "               '2002-01-05', '2002-01-06', '2002-01-07', '2002-01-08',\n",
      "               '2002-01-09', '2002-01-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia08156.csv: DatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03', '2001-01-04',\n",
      "               '2001-01-05', '2001-01-06', '2001-01-07', '2001-01-08',\n",
      "               '2001-01-09', '2001-01-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia19067.csv: DatetimeIndex(['2003-10-31', '2003-11-01', '2003-11-02', '2003-11-03',\n",
      "               '2003-11-04', '2003-11-05', '2003-11-06', '2003-11-07',\n",
      "               '2003-11-08', '2003-11-09'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia19098.csv: DatetimeIndex(['2000-03-28', '2003-08-01', '2003-08-02', '2003-08-03',\n",
      "               '2003-08-04', '2003-08-05', '2003-08-06', '2003-08-07',\n",
      "               '2003-08-08', '2003-08-09'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia22026.csv: DatetimeIndex(['2000-04-01', '2000-04-02', '2000-04-03', '2000-04-04',\n",
      "               '2000-04-05', '2000-04-06', '2000-04-07', '2000-04-08',\n",
      "               '2000-04-09', '2000-04-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia20143.csv: DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\n",
      "               '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08',\n",
      "               '2000-01-09', '2000-01-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia15305.csv: DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n",
      "               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n",
      "               '2003-01-09', '2003-01-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia30185.csv: DatetimeIndex(['2004-12-06', '2004-12-07', '2006-12-19', '2006-12-20',\n",
      "               '2006-12-21', '2006-12-22', '2006-12-23', '2006-12-24',\n",
      "               '2006-12-25', '2006-12-26'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia14033.csv: DatetimeIndex(['2001-05-30', '2004-04-01', '2004-04-02', '2004-04-03',\n",
      "               '2004-04-04', '2004-04-05', '2004-04-06', '2004-04-07',\n",
      "               '2004-04-08', '2004-04-09'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Fechas faltantes en dia02137.csv: DatetimeIndex(['2001-09-01', '2001-09-02', '2001-09-03', '2001-09-04',\n",
      "               '2001-09-05', '2001-09-06', '2001-09-07', '2001-09-08',\n",
      "               '2001-09-09', '2001-09-10'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Función para verificar presencia completa de fechas en un archivo CSV\n",
    "def verificar_fechas_completas(file_path, fecha_inicio, fecha_fin):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Fecha' in df.columns:\n",
    "            df['Fecha'] = pd.to_datetime(df['Fecha'], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Columna 'Fecha' faltante en {file_path}\")\n",
    "            return False, []\n",
    "        \n",
    "        # Crear un rango de fechas completo\n",
    "        fechas_completas = pd.date_range(start=fecha_inicio, end=fecha_fin, freq='D')\n",
    "        \n",
    "        # Verificar si todas las fechas están presentes en el DataFrame\n",
    "        fechas_df = df['Fecha'].dropna().unique()\n",
    "        fechas_faltantes = fechas_completas.difference(fechas_df)\n",
    "        \n",
    "        return len(fechas_faltantes) == 0, fechas_faltantes\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {file_path}: {e}\")\n",
    "        return False, []\n",
    "\n",
    "fecha_inicio = pd.Timestamp('2000-01-01')\n",
    "fecha_fin = pd.Timestamp('2020-12-31')\n",
    "\n",
    "# Verificar presencia completa de fechas en los archivos CSV\n",
    "archivos_csv = [f for f in os.listdir(carpeta_csv) if f.startswith('dia') and f.endswith('.csv')]\n",
    "\n",
    "resultados_verificacion = {}\n",
    "for archivo in tqdm(archivos_csv, desc=\"Verificando fechas completas en CSVs\"):\n",
    "    file_path = os.path.join(carpeta_csv, archivo)\n",
    "    fechas_completas, fechas_faltantes = verificar_fechas_completas(file_path, fecha_inicio, fecha_fin)\n",
    "    resultados_verificacion[archivo] = (fechas_completas, fechas_faltantes)\n",
    "\n",
    "# Mostrar resultados\n",
    "archivos_incompletos = [archivo for archivo, (fechas_completas, _) in resultados_verificacion.items() if not fechas_completas]\n",
    "print(f\"Cantidad de archivos CSV con fechas incompletas: {len(archivos_incompletos)}\")\n",
    "print(\"Archivos CSV con fechas incompletas:\", archivos_incompletos[:10])  # Mostrar los primeros 10 archivos con fechas incompletas\n",
    "\n",
    "# Mostrar las fechas faltantes para los primeros archivos con fechas incompletas\n",
    "for archivo in archivos_incompletos[:10]:\n",
    "    _, fechas_faltantes = resultados_verificacion[archivo]\n",
    "    print(f\"Fechas faltantes en {archivo}: {fechas_faltantes[:10]}\")  # Mostrar las primeras 10 fechas faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd1a0a-fb3e-46ac-b05c-8b74d440e834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
